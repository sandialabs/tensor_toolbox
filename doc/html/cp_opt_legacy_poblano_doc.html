
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>All-at-once optimization for CP tensor decomposition (with Poblano)</title><meta name="generator" content="MATLAB 9.11"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2022-08-15"><meta name="DC.source" content="cp_opt_legacy_poblano_doc.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; }

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }
span.typesection { color:#A0522D }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>All-at-once optimization for CP tensor decomposition (with Poblano)</h1><!--introduction--><p>We explain how to use <tt>cp_opt_legacy</tt> with the Poblano toolbox. The default is to use L-BFGS-B (not Poblano), which is described <a href="cp_opt_legacy_doc.html">here</a>.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Newer version available</a></li><li><a href="#2">Poblano Optimization Toolbox</a></li><li><a href="#3">Create an example problem.</a></li><li><a href="#4">Create initial guess using 'nvecs'</a></li><li><a href="#5">Set up the optimization parameters</a></li><li><a href="#6">Call the <tt>cp_opt_legacy</tt> method</a></li><li><a href="#7">Check the output</a></li><li><a href="#9">Evaluate the output</a></li><li><a href="#10">Overfitting example</a></li></ul></div><h2 id="1">Newer version available</h2><p>This documentation is for the legacy implementation of CP-OPT. We recommend using the newer version, described <a href="cp_opt_doc.html">here</a>.</p><h2 id="2">Poblano Optimization Toolbox</h2><p>Check that you have Poblano 1.1 (or later) installed. The output of your 'ver' command should look something like the following.</p><pre class="codeinput">ver
</pre><pre class="codeoutput">-----------------------------------------------------------------------------------------------------
MATLAB Version: 9.11.0.1873467 (R2021b) Update 3
MATLAB License Number: 40941043
Operating System: Microsoft Windows 11 Pro Version 10.0 (Build 22000)
Java Version: Java 1.8.0_202-b08 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode
-----------------------------------------------------------------------------------------------------
MATLAB                                                Version 9.11        (R2021b) 
Deep Learning Toolbox                                 Version 14.3        (R2021b) 
Image Processing Toolbox                              Version 11.4        (R2021b) 
Mapping Toolbox                                       Version 5.2         (R2021b) 
Optimization Toolbox                                  Version 9.2         (R2021b) 
Parallel Computing Toolbox                            Version 7.5         (R2021b) 
Poblano Toolbox (Sandia National Labs)                Version 1.2                  
Statistics and Machine Learning Toolbox               Version 12.2        (R2021b) 
Symbolic Math Toolbox                                 Version 9.0         (R2021b) 
Tensor Toolbox (Sandia National Labs)                 Version 3.3.a-dev    (R2022a)
Text Analytics Toolbox                                Version 1.8         (R2021b) 
Wavelet Toolbox                                       Version 6.0         (R2021b) 
</pre><h2 id="3">Create an example problem.</h2><p>Create an example 50 x 40 x 30 tensor with rank 5 and add 10% noise.</p><pre class="codeinput">R = 5;
info = create_problem(<span class="string">'Size'</span>, [50 40 30], <span class="string">'Num_Factors'</span>, R, <span class="string">'Noise'</span>, 0.10);
X = info.Data;
M_true = info.Soln;
</pre><h2 id="4">Create initial guess using 'nvecs'</h2><pre class="codeinput">M_init = create_guess(<span class="string">'Data'</span>, X, <span class="string">'Num_Factors'</span>, R, <span class="keyword">...</span>
    <span class="string">'Factor_Generator'</span>, <span class="string">'nvecs'</span>);
</pre><h2 id="5">Set up the optimization parameters</h2><p>It's genearlly a good idea to consider the parameters of the optimization method. The default options may be either too stringent or not stringent enough. The most important options to consider are detailed here.</p><pre class="codeinput"><span class="comment">% Get the defaults</span>
ncg_opts = ncg(<span class="string">'defaults'</span>);
<span class="comment">% Tighten the stop tolerance (norm of gradient). This is often too large.</span>
ncg_opts.StopTol = 1.0e-6;
<span class="comment">% Tighten relative change in function value tolearnce. This is often too large.</span>
ncg_opts.RelFuncTol = 1.0e-20;
<span class="comment">% Increase the number of iterations.</span>
ncg_opts.MaxIters = 10^4;
<span class="comment">% Only display every 10th iteration</span>
ncg_opts.DisplayIters = 10;
<span class="comment">% Display the final set of options</span>
ncg_opts
</pre><pre class="codeoutput">
ncg_opts = 

  struct with fields:

                   Display: 'iter'
              DisplayIters: 10
           LineSearch_ftol: 1.0000e-04
           LineSearch_gtol: 0.0100
    LineSearch_initialstep: 1
         LineSearch_maxfev: 20
         LineSearch_method: 'more-thuente'
         LineSearch_stpmax: 1.0000e+15
         LineSearch_stpmin: 1.0000e-15
           LineSearch_xtol: 1.0000e-15
              MaxFuncEvals: 10000
                  MaxIters: 10000
                RelFuncTol: 1.0000e-20
              RestartIters: 20
                 RestartNW: 0
              RestartNWTol: 0.1000
                   StopTol: 1.0000e-06
                 TraceFunc: 0
            TraceFuncEvals: 0
                 TraceGrad: 0
             TraceGradNorm: 0
              TraceRelFunc: 0
                    TraceX: 0
                    Update: 'PR'

</pre><h2 id="6">Call the <tt>cp_opt_legacy</tt> method</h2><p>Here is an example call to the cp_opt_legacy method. By default, each iteration prints the least squares fit function value (being minimized) and the norm of the gradient. The meaning of any line search warnings can be checked via <a href="matlab:doc('cvsrch')">doc cvsrch</a>.</p><pre class="codeinput">[M,~,output] = cp_opt_legacy(X, R, <span class="string">'init'</span>, M_init, <span class="keyword">...</span>
    <span class="string">'opt'</span>, <span class="string">'ncg'</span>, <span class="string">'opt_options'</span>, ncg_opts);
</pre><pre class="codeoutput"> Iter  FuncEvals       F(X)          ||G(X)||/N        
------ --------- ---------------- ----------------
     0         1   35904.25417478       0.51128623
    10        75   10095.75055905       0.42280680
    20       146     566.08955198       0.83490912
    30       195     352.93952570       0.02770418
    40       226     352.72925483       0.00410882
    50       253     352.72136899       0.00105734
    60       275     352.72056707       0.00008153
    70       295     352.72055849       0.00001580
    80       315     352.72055833       0.00000346
    90       335     352.72055831       0.00000126
    95       345     352.72055831       0.00000068
</pre><h2 id="7">Check the output</h2><p>It's important to check the output of the optimization method. In particular, it's worthwhile to check the exit flag. A zero (0) indicates successful termination with the gradient smaller than the specified StopTol, and a three (3) indicates a successful termination where the change in function value is less than RelFuncTol. The meaning of any other flags can be checked via <a href="matlab:doc('poblano_params')">doc poblano_params</a>.</p><pre class="codeinput">exitflag = output.ExitFlag
</pre><pre class="codeoutput">
exitflag =

     0

</pre><p>The fit is the percentage of the data that is explained by the model. Because we have noise, we do not expect the fit to be perfect.</p><pre class="codeinput">fit = output.Fit
</pre><pre class="codeoutput">
fit =

   99.0191

</pre><h2 id="9">Evaluate the output</h2><p>We can "score" the similarity of the model computed by CP and compare that with the truth. The <tt>score</tt> function on ktensor's gives a score in [0,1]  with 1 indicating a perfect match. Because we have noise, we do not expect the fit to be perfect. See <a href="matlab:doc('ktensor/score')">doc score</a> for more details.</p><pre class="codeinput">scr = score(M,M_true)
</pre><pre class="codeoutput">
scr =

    0.9965

</pre><h2 id="10">Overfitting example</h2><p>Consider the case where we don't know R in advance. We might guess too high. Here we show a case where we guess R+1 factors rather than R.</p><pre class="codeinput"><span class="comment">% Generate initial guess of the corret size</span>
M_plus_init = create_guess(<span class="string">'Data'</span>, X, <span class="string">'Num_Factors'</span>, R+1, <span class="keyword">...</span>
    <span class="string">'Factor_Generator'</span>, <span class="string">'nvecs'</span>);
</pre><pre class="codeinput"><span class="comment">% Loosen the stop tolerance (norm of gradient).</span>
ncg_opts.StopTol = 1.0e-2;
</pre><pre class="codeinput"><span class="comment">% Run the algorithm</span>
[M_plus,~,output] = cp_opt_legacy(X, R+1, <span class="string">'init'</span>, M_plus_init, <span class="keyword">...</span>
    <span class="string">'opt'</span>, <span class="string">'ncg'</span>, <span class="string">'opt_options'</span>, ncg_opts);
exitflag = output.ExitFlag
fit = output.Fit
</pre><pre class="codeoutput"> Iter  FuncEvals       F(X)          ||G(X)||/N        
------ --------- ---------------- ----------------
     0         1   35904.55558871       0.42608245
    10        75   10095.80182770       0.35237040
    20       146     565.93369157       0.69580877
    30       195     352.68136727       0.02334301
    32       202     352.52902639       0.00994303

exitflag =

     0


fit =

   99.0196

</pre><pre class="codeinput"><span class="comment">% Check the answer (1 is perfect)</span>
scr = score(M_plus, M_true)
</pre><pre class="codeoutput">
scr =

    0.9943

</pre><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2021b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% All-at-once optimization for CP tensor decomposition (with Poblano)
% We explain how to use |cp_opt_legacy| with the Poblano toolbox. The default is
% to use L-BFGS-B (not Poblano), which is described <cp_opt_legacy_doc.html here>.

%% Newer version available
% This documentation is for the legacy implementation of CP-OPT. We
% recommend using the newer version, described <cp_opt_doc.html here>.

%% Poblano Optimization Toolbox
% Check that you have Poblano 1.1 (or later) installed. The output of your 'ver'
% command should look something like the following.
ver

%% Create an example problem. 
% Create an example 50 x 40 x 30 tensor with rank 5 and add 10% noise.
R = 5;
info = create_problem('Size', [50 40 30], 'Num_Factors', R, 'Noise', 0.10);
X = info.Data;
M_true = info.Soln;

%% Create initial guess using 'nvecs'
M_init = create_guess('Data', X, 'Num_Factors', R, ...
    'Factor_Generator', 'nvecs');


%% Set up the optimization parameters
% It's genearlly a good idea to consider the parameters of the optimization
% method. The default options may be either too stringent or not stringent
% enough. The most important options to consider are detailed here. 

% Get the defaults
ncg_opts = ncg('defaults');
% Tighten the stop tolerance (norm of gradient). This is often too large.
ncg_opts.StopTol = 1.0e-6;
% Tighten relative change in function value tolearnce. This is often too large.
ncg_opts.RelFuncTol = 1.0e-20;
% Increase the number of iterations. 
ncg_opts.MaxIters = 10^4;
% Only display every 10th iteration
ncg_opts.DisplayIters = 10;
% Display the final set of options
ncg_opts

%% Call the |cp_opt_legacy| method
% Here is an example call to the cp_opt_legacy method. By default, each iteration
% prints the least squares fit function value (being minimized) and the
% norm of the gradient. The meaning of any line search warnings
% can be checked via <matlab:doc('cvsrch') doc cvsrch>.
[M,~,output] = cp_opt_legacy(X, R, 'init', M_init, ...
    'opt', 'ncg', 'opt_options', ncg_opts);

%% Check the output
% It's important to check the output of the optimization method. In
% particular, it's worthwhile to check the exit flag. 
% A zero (0) indicates successful termination with the gradient smaller
% than the specified StopTol, and a three (3) indicates a successful
% termination where the change in function value is less than RelFuncTol.
% The meaning of any other flags can be checked via 
% <matlab:doc('poblano_params') doc poblano_params>. 
exitflag = output.ExitFlag

%%
% The fit is the percentage of the data that is explained by the model.
% Because we have noise, we do not expect the fit to be perfect.
fit = output.Fit

%% Evaluate the output
% We can "score" the similarity of the model computed by CP and compare
% that with the truth. The |score| function on ktensor's gives a score in
% [0,1]  with 1 indicating a perfect match. Because we have noise, we do
% not expect the fit to be perfect. See <matlab:doc('ktensor/score') doc
% score> for more details.
scr = score(M,M_true)

%% Overfitting example
% Consider the case where we don't know R in advance. We might guess too
% high. Here we show a case where we guess R+1 factors rather than R.

% Generate initial guess of the corret size
M_plus_init = create_guess('Data', X, 'Num_Factors', R+1, ...
    'Factor_Generator', 'nvecs');

%% 

% Loosen the stop tolerance (norm of gradient). 
ncg_opts.StopTol = 1.0e-2;

%%

% Run the algorithm
[M_plus,~,output] = cp_opt_legacy(X, R+1, 'init', M_plus_init, ...
    'opt', 'ncg', 'opt_options', ncg_opts);
exitflag = output.ExitFlag
fit = output.Fit


%%

% Check the answer (1 is perfect)
scr = score(M_plus, M_true)


##### SOURCE END #####
--></body></html>